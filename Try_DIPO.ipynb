{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecc488ed-5f41-4978-9e33-93cc902d04eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from agent.DiPo import DiPo\n",
    "from agent.replay_memory import ReplayMemory, DiffusionMemory\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import gym\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6777e5-518b-4035-b790-a86718c2dfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mujoco/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#env = gym.make(\"Ant-v4\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m env\u001b[38;5;241m=\u001b[39m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHopper-v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mujoco/lib/python3.9/site-packages/gym/envs/registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    645\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/envs/mujoco/lib/python3.9/site-packages/gym/envs/mujoco/hopper_v3.py:82\u001b[0m, in \u001b[0;36mHopperEnv.__init__\u001b[0;34m(self, xml_file, forward_reward_weight, ctrl_cost_weight, healthy_reward, terminate_when_unhealthy, healthy_state_range, healthy_z_range, healthy_angle_range, reset_noise_scale, exclude_current_positions_from_observation, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     observation_space \u001b[38;5;241m=\u001b[39m Box(\n\u001b[1;32m     79\u001b[0m         low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, high\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m \u001b[43mMuJocoPyEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/mujoco/lib/python3.9/site-packages/gym/envs/mujoco/mujoco_env.py:186\u001b[0m, in \u001b[0;36mMuJocoPyEnv.__init__\u001b[0;34m(self, model_path, frame_skip, observation_space, render_mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    176\u001b[0m     model_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     camera_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m ):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MUJOCO_PY_IMPORT_ERROR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMUJOCO_PY_IMPORT_ERROR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    190\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis version of the mujoco environments depends \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon the mujoco-py bindings, which are no longer maintained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou are trying to precisely replicate previous works).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    199\u001b[0m         model_path,\n\u001b[1;32m    200\u001b[0m         frame_skip,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m         camera_name,\n\u001b[1;32m    207\u001b[0m     )\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "#env = gym.make(\"Ant-v4\")\n",
    "\n",
    "env=gym.make(\"Hopper-v3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43155fc5-40fd-4075-8291-82250bb49ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "373d73bf-ad98-49d3-ac1d-1cee13bb4b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--cuda'], dest='cuda', nargs=None, const=None, default='cuda:0', type=None, choices=None, required=False, help='run on CUDA (default: cuda:0)', metavar=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Diffusion Policy')\n",
    "parser.add_argument('--env_name', default=\"Hopper-v3\",\n",
    "                    help='Mujoco Gym environment (default: Hopper-v3)')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='N',\n",
    "                    help='random seed (default: 0)')\n",
    "\n",
    "parser.add_argument('--num_steps', type=int, default=1000000, metavar='N',\n",
    "                    help='env timesteps (default: 1000000)')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=256, metavar='N',\n",
    "                    help='batch size (default: 256)')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor for reward (default: 0.99)')\n",
    "parser.add_argument('--tau', type=float, default=0.005, metavar='G',\n",
    "                    help='target smoothing coefficient(Ï„) (default: 0.005)')\n",
    "parser.add_argument('--update_actor_target_every', type=int, default=1, metavar='N',\n",
    "                    help='update actor target per iteration (default: 1)')\n",
    "\n",
    "parser.add_argument(\"--policy_type\", type=str, default=\"Diffusion\", metavar='S',\n",
    "                    help=\"Diffusion, VAE or MLP\")\n",
    "parser.add_argument(\"--beta_schedule\", type=str, default=\"cosine\", metavar='S',\n",
    "                    help=\"linear, cosine or vp\")\n",
    "parser.add_argument('--n_timesteps', type=int, default=100, metavar='N',\n",
    "                    help='diffusion timesteps (default: 100)')\n",
    "parser.add_argument('--diffusion_lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='diffusion learning rate (default: 0.0003)')\n",
    "parser.add_argument('--critic_lr', type=float, default=0.0003, metavar='G',\n",
    "                    help='critic learning rate (default: 0.0003)')\n",
    "parser.add_argument('--action_lr', type=float, default=0.03, metavar='G',\n",
    "                    help='diffusion learning rate (default: 0.03)')\n",
    "parser.add_argument('--noise_ratio', type=float, default=1.0, metavar='G',\n",
    "                    help='noise ratio in sample process (default: 1.0)')\n",
    "\n",
    "parser.add_argument('--action_gradient_steps', type=int, default=20, metavar='N',\n",
    "                    help='action gradient steps (default: 20)')\n",
    "parser.add_argument('--ratio', type=float, default=0.1, metavar='G',\n",
    "                    help='the ratio of action grad norm to action_dim (default: 0.1)')\n",
    "parser.add_argument('--ac_grad_norm', type=float, default=2.0, metavar='G',\n",
    "                    help='actor and critic grad norm (default: 1.0)')\n",
    "\n",
    "parser.add_argument('--cuda', default='cuda:0',\n",
    "                    help='run on CUDA (default: cuda:0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c8817b-0f28-4a7e-a953-91d410ae8acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args=parser.parse_known_args()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb0c421-82a5-4972-87c7-3b44eafa1aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args.env_name=\"Ant-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a2fea-d4aa-421a-9ff5-8737cd062a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd816cc-7786-48e8-8b9b-ef8c5e5ac91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(args.cuda)\n",
    "\n",
    "dir = \"record\"\n",
    "# dir = \"test\"\n",
    "log_dir = os.path.join(dir, f'{args.env_name}', f'policy_type={args.policy_type}', f'ratio={args.ratio}', f'seed={args.seed}')\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "# Initial environment\n",
    "env = gym.make(args.env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3fd9f4d-4f3f-4010-92cb-f4bc887bd73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "state_size = int(np.prod(env.observation_space.shape))\n",
    "action_size = int(np.prod(env.action_space.shape))\n",
    "print(action_size)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0c4638-d60b-4cd5-949d-21197166b61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory_size = 1e6\n",
    "num_steps = args.num_steps\n",
    "start_steps = 10000\n",
    "eval_interval = 10000\n",
    "updates_per_step = 1\n",
    "batch_size = args.batch_size\n",
    "log_interval = 10\n",
    "\n",
    "memory = ReplayMemory(state_size, action_size, memory_size, device)\n",
    "diffusion_memory = DiffusionMemory(state_size, action_size, memory_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c858f881-4673-4863-9ab6-1c17f272bb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = DiPo(args, state_size, env.action_space, memory, diffusion_memory, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b67ee6-51e9-425e-9048-187a3ea8ce05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, writer, steps):\n",
    "    print(\"eva\")\n",
    "    episodes = 10\n",
    "    returns = np.zeros((episodes,), dtype=np.float32)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"eva_i\",i)\n",
    "        state = env.reset()\n",
    "        episode_reward = 0.\n",
    "        done = False\n",
    "        state=state[0]\n",
    "        start=0\n",
    "        while not done and start<=1000:\n",
    "            start+=1\n",
    "            #print(\"start\",start)\n",
    "            #print(\"step1\")\n",
    "            action = agent.sample_action(state, eval=True)\n",
    "            #print(\"step2\")\n",
    "            next_state, reward, done, _,info = env.step(action)\n",
    "            #print(\"done\",done)\n",
    "            episode_reward += reward\n",
    "            #print(\"next_state\",next_state)\n",
    "            \"\"\"\n",
    "            if(len(next_state.shape)==1):\n",
    "                state = np.expand_dims(next_state,axis=0)\n",
    "            else:\n",
    "            \"\"\"\n",
    "            state=next_state\n",
    "        returns[i] = episode_reward\n",
    "\n",
    "    mean_return = np.mean(returns)\n",
    "\n",
    "    writer.add_scalar(\n",
    "            'reward/test', mean_return, steps)\n",
    "    print('-' * 60)\n",
    "    print(f'Num steps: {steps:<5}  '\n",
    "              f'reward: {mean_return:<5.1f}')\n",
    "    print('-' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79dc103-d14d-4b2b-b07f-17198f0a493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86040c37-3fcc-4d1e-826c-64ab44a01b42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 37\n",
      "episode: 1     episode steps: 37    reward: -2.3 \n",
      "step 52\n",
      "episode: 2     episode steps: 15    reward: -4.2 \n",
      "step 249\n",
      "episode: 3     episode steps: 197   reward: 8.5  \n",
      "step 310\n",
      "episode: 4     episode steps: 61    reward: 35.3 \n",
      "step 344\n",
      "episode: 5     episode steps: 34    reward: -7.6 \n",
      "step 367\n",
      "episode: 6     episode steps: 23    reward: -11.5\n",
      "step 458\n",
      "episode: 7     episode steps: 91    reward: -62.4\n",
      "step 514\n",
      "episode: 8     episode steps: 56    reward: -44.2\n",
      "step 548\n",
      "episode: 9     episode steps: 34    reward: -4.4 \n",
      "step 581\n",
      "episode: 10    episode steps: 33    reward: -11.4\n",
      "step 634\n",
      "episode: 11    episode steps: 53    reward: -9.2 \n",
      "step 673\n",
      "episode: 12    episode steps: 39    reward: -16.4\n",
      "step 706\n",
      "episode: 13    episode steps: 33    reward: -6.9 \n",
      "step 742\n",
      "episode: 14    episode steps: 36    reward: -6.2 \n",
      "step 827\n",
      "episode: 15    episode steps: 85    reward: -129.2\n",
      "step 886\n",
      "episode: 16    episode steps: 59    reward: -10.9\n",
      "step 1047\n",
      "episode: 17    episode steps: 161   reward: -180.2\n",
      "step 2048\n",
      "episode: 18    episode steps: 1001  reward: -410.2\n",
      "step 2110\n",
      "episode: 19    episode steps: 62    reward: -23.4\n",
      "step 2142\n",
      "episode: 20    episode steps: 32    reward: -14.0\n",
      "step 2178\n",
      "episode: 21    episode steps: 36    reward: -28.3\n",
      "step 2288\n",
      "episode: 22    episode steps: 110   reward: 26.3 \n",
      "step 2429\n",
      "episode: 23    episode steps: 141   reward: -25.7\n",
      "step 2487\n",
      "episode: 24    episode steps: 58    reward: -30.3\n",
      "step 2560\n",
      "episode: 25    episode steps: 73    reward: -62.2\n",
      "step 2600\n",
      "episode: 26    episode steps: 40    reward: -25.0\n",
      "step 2717\n",
      "episode: 27    episode steps: 117   reward: -68.1\n",
      "step 2752\n",
      "episode: 28    episode steps: 35    reward: 1.6  \n",
      "step 2918\n",
      "episode: 29    episode steps: 166   reward: -16.6\n",
      "step 3021\n",
      "episode: 30    episode steps: 103   reward: -27.8\n",
      "step 4022\n",
      "episode: 31    episode steps: 1001  reward: -387.7\n",
      "step 4037\n",
      "episode: 32    episode steps: 15    reward: 6.5  \n",
      "step 4112\n",
      "episode: 33    episode steps: 75    reward: -64.5\n",
      "step 4149\n",
      "episode: 34    episode steps: 37    reward: -20.7\n",
      "step 4214\n",
      "episode: 35    episode steps: 65    reward: -35.6\n",
      "step 4336\n",
      "episode: 36    episode steps: 122   reward: -84.5\n",
      "step 4614\n",
      "episode: 37    episode steps: 278   reward: -68.9\n",
      "step 4634\n",
      "episode: 38    episode steps: 20    reward: -8.2 \n",
      "step 4702\n",
      "episode: 39    episode steps: 68    reward: -49.0\n",
      "step 5703\n",
      "episode: 40    episode steps: 1001  reward: -359.4\n",
      "step 5727\n",
      "episode: 41    episode steps: 24    reward: -1.3 \n",
      "step 5841\n",
      "episode: 42    episode steps: 114   reward: -134.9\n",
      "step 5939\n",
      "episode: 43    episode steps: 98    reward: -5.2 \n",
      "step 6320\n",
      "episode: 44    episode steps: 381   reward: -87.9\n",
      "step 6347\n",
      "episode: 45    episode steps: 27    reward: -0.1 \n",
      "step 6417\n",
      "episode: 46    episode steps: 70    reward: 19.9 \n",
      "step 6505\n",
      "episode: 47    episode steps: 88    reward: -51.2\n",
      "step 6579\n",
      "episode: 48    episode steps: 74    reward: -30.0\n",
      "step 6680\n",
      "episode: 49    episode steps: 101   reward: -73.5\n",
      "step 7681\n",
      "episode: 50    episode steps: 1001  reward: -381.5\n",
      "step 7821\n",
      "episode: 51    episode steps: 140   reward: -68.7\n",
      "step 7861\n",
      "episode: 52    episode steps: 40    reward: 0.7  \n",
      "step 8862\n",
      "episode: 53    episode steps: 1001  reward: -336.4\n",
      "step 9002\n",
      "episode: 54    episode steps: 140   reward: -26.8\n",
      "step 9045\n",
      "episode: 55    episode steps: 43    reward: -17.5\n",
      "step 9097\n",
      "episode: 56    episode steps: 52    reward: -8.1 \n",
      "step 9207\n",
      "episode: 57    episode steps: 110   reward: -27.0\n",
      "step 9236\n",
      "episode: 58    episode steps: 29    reward: 9.9  \n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 10000  reward: -64.2\n",
      "------------------------------------------------------------\n",
      "step 10000\n",
      "episode: 59    episode steps: 764   reward: -295.0\n",
      "step 10018\n",
      "episode: 60    episode steps: 18    reward: -30.9\n",
      "step 10079\n",
      "episode: 61    episode steps: 61    reward: -69.1\n",
      "step 10125\n",
      "episode: 62    episode steps: 46    reward: -64.0\n",
      "step 10144\n",
      "episode: 63    episode steps: 19    reward: -14.6\n",
      "step 10170\n",
      "episode: 64    episode steps: 26    reward: -42.8\n",
      "step 10187\n",
      "episode: 65    episode steps: 17    reward: -20.2\n",
      "step 10248\n",
      "episode: 66    episode steps: 61    reward: -24.4\n",
      "step 10289\n",
      "episode: 67    episode steps: 41    reward: -19.3\n",
      "step 10308\n",
      "episode: 68    episode steps: 19    reward: -10.2\n",
      "step 10320\n",
      "episode: 69    episode steps: 12    reward: -10.1\n",
      "step 10350\n",
      "episode: 70    episode steps: 30    reward: -30.0\n",
      "step 10460\n",
      "episode: 71    episode steps: 110   reward: -53.0\n",
      "step 10617\n",
      "episode: 72    episode steps: 157   reward: -77.4\n",
      "step 10696\n",
      "episode: 73    episode steps: 79    reward: -65.2\n",
      "step 10710\n",
      "episode: 74    episode steps: 14    reward: -15.6\n",
      "step 10742\n",
      "episode: 75    episode steps: 32    reward: -14.2\n",
      "step 11743\n",
      "episode: 76    episode steps: 1001  reward: -720.5\n",
      "step 11759\n",
      "episode: 77    episode steps: 16    reward: -15.1\n",
      "step 11792\n",
      "episode: 78    episode steps: 33    reward: -11.6\n",
      "step 11951\n",
      "episode: 79    episode steps: 159   reward: -119.8\n",
      "step 12125\n",
      "episode: 80    episode steps: 174   reward: -165.0\n",
      "step 13126\n",
      "episode: 81    episode steps: 1001  reward: -658.1\n",
      "step 13982\n",
      "episode: 82    episode steps: 856   reward: -579.6\n",
      "step 14562\n",
      "episode: 83    episode steps: 580   reward: -366.3\n",
      "step 14631\n",
      "episode: 84    episode steps: 69    reward: -47.4\n",
      "step 15632\n",
      "episode: 85    episode steps: 1001  reward: -551.9\n",
      "step 15654\n",
      "episode: 86    episode steps: 22    reward: -27.8\n",
      "step 15680\n",
      "episode: 87    episode steps: 26    reward: 16.2 \n",
      "step 15759\n",
      "episode: 88    episode steps: 79    reward: -35.0\n",
      "step 15854\n",
      "episode: 89    episode steps: 95    reward: -56.8\n",
      "step 16100\n",
      "episode: 90    episode steps: 246   reward: -226.7\n",
      "step 16114\n",
      "episode: 91    episode steps: 14    reward: -25.1\n",
      "step 16140\n",
      "episode: 92    episode steps: 26    reward: -28.8\n",
      "step 16227\n",
      "episode: 93    episode steps: 87    reward: -78.8\n",
      "step 16239\n",
      "episode: 94    episode steps: 12    reward: -1.0 \n",
      "step 16303\n",
      "episode: 95    episode steps: 64    reward: -0.9 \n",
      "step 16367\n",
      "episode: 96    episode steps: 64    reward: -61.7\n",
      "step 16453\n",
      "episode: 97    episode steps: 86    reward: -101.3\n",
      "step 16508\n",
      "episode: 98    episode steps: 55    reward: -32.1\n",
      "step 16580\n",
      "episode: 99    episode steps: 72    reward: -57.3\n",
      "step 16750\n",
      "episode: 100   episode steps: 170   reward: -228.9\n",
      "step 16769\n",
      "episode: 101   episode steps: 19    reward: -14.4\n",
      "step 16784\n",
      "episode: 102   episode steps: 15    reward: -17.4\n",
      "step 16804\n",
      "episode: 103   episode steps: 20    reward: -19.7\n",
      "step 16864\n",
      "episode: 104   episode steps: 60    reward: -37.4\n",
      "step 16916\n",
      "episode: 105   episode steps: 52    reward: -35.1\n",
      "step 16954\n",
      "episode: 106   episode steps: 38    reward: -25.5\n",
      "step 17955\n",
      "episode: 107   episode steps: 1001  reward: -648.9\n",
      "step 18048\n",
      "episode: 108   episode steps: 93    reward: -70.1\n",
      "step 18076\n",
      "episode: 109   episode steps: 28    reward: -15.4\n",
      "step 18113\n",
      "episode: 110   episode steps: 37    reward: -17.0\n",
      "step 18160\n",
      "episode: 111   episode steps: 47    reward: -19.2\n",
      "step 18204\n",
      "episode: 112   episode steps: 44    reward: -54.7\n",
      "step 18235\n",
      "episode: 113   episode steps: 31    reward: -24.0\n",
      "step 18250\n",
      "episode: 114   episode steps: 15    reward: -15.8\n",
      "step 18293\n",
      "episode: 115   episode steps: 43    reward: -25.2\n",
      "step 18309\n",
      "episode: 116   episode steps: 16    reward: -3.1 \n",
      "step 18334\n",
      "episode: 117   episode steps: 25    reward: -23.8\n",
      "step 18385\n",
      "episode: 118   episode steps: 51    reward: -20.8\n",
      "step 18418\n",
      "episode: 119   episode steps: 33    reward: -15.8\n",
      "step 18517\n",
      "episode: 120   episode steps: 99    reward: -135.2\n",
      "step 18548\n",
      "episode: 121   episode steps: 31    reward: -19.8\n",
      "step 18594\n",
      "episode: 122   episode steps: 46    reward: -40.2\n",
      "step 18604\n",
      "episode: 123   episode steps: 10    reward: -7.4 \n",
      "step 18625\n",
      "episode: 124   episode steps: 21    reward: 0.1  \n",
      "step 18778\n",
      "episode: 125   episode steps: 153   reward: -123.8\n",
      "step 18808\n",
      "episode: 126   episode steps: 30    reward: 1.7  \n",
      "step 18903\n",
      "episode: 127   episode steps: 95    reward: -54.4\n",
      "step 19060\n",
      "episode: 128   episode steps: 157   reward: -117.3\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 20000  reward: 135.7\n",
      "------------------------------------------------------------\n",
      "step 20000\n",
      "episode: 129   episode steps: 940   reward: -597.9\n",
      "step 20023\n",
      "episode: 130   episode steps: 23    reward: -34.5\n",
      "step 20046\n",
      "episode: 131   episode steps: 23    reward: -7.7 \n",
      "step 20085\n",
      "episode: 132   episode steps: 39    reward: -11.6\n",
      "step 20106\n",
      "episode: 133   episode steps: 21    reward: -4.4 \n",
      "step 20130\n",
      "episode: 134   episode steps: 24    reward: -10.5\n",
      "step 20149\n",
      "episode: 135   episode steps: 19    reward: -7.4 \n",
      "step 20179\n",
      "episode: 136   episode steps: 30    reward: -24.9\n",
      "step 20237\n",
      "episode: 137   episode steps: 58    reward: -43.9\n",
      "step 20414\n",
      "episode: 138   episode steps: 177   reward: -170.1\n",
      "step 20558\n",
      "episode: 139   episode steps: 144   reward: -132.9\n",
      "step 20628\n",
      "episode: 140   episode steps: 70    reward: 9.0  \n",
      "step 20652\n",
      "episode: 141   episode steps: 24    reward: -33.2\n",
      "step 21653\n",
      "episode: 142   episode steps: 1001  reward: -526.8\n",
      "step 22654\n",
      "episode: 143   episode steps: 1001  reward: -457.4\n",
      "step 22850\n",
      "episode: 144   episode steps: 196   reward: -128.1\n",
      "step 22895\n",
      "episode: 145   episode steps: 45    reward: -38.9\n",
      "step 23896\n",
      "episode: 146   episode steps: 1001  reward: -352.5\n",
      "step 23983\n",
      "episode: 147   episode steps: 87    reward: -68.2\n",
      "step 24021\n",
      "episode: 148   episode steps: 38    reward: -24.4\n",
      "step 24127\n",
      "episode: 149   episode steps: 106   reward: -68.6\n",
      "step 24149\n",
      "episode: 150   episode steps: 22    reward: -14.1\n",
      "step 24165\n",
      "episode: 151   episode steps: 16    reward: -18.1\n",
      "step 24174\n",
      "episode: 152   episode steps: 9     reward: -0.9 \n",
      "step 24224\n",
      "episode: 153   episode steps: 50    reward: -19.5\n",
      "step 24247\n",
      "episode: 154   episode steps: 23    reward: -12.5\n",
      "step 24299\n",
      "episode: 155   episode steps: 52    reward: -5.4 \n",
      "step 24406\n",
      "episode: 156   episode steps: 107   reward: -38.7\n",
      "step 24469\n",
      "episode: 157   episode steps: 63    reward: -30.5\n",
      "step 24729\n",
      "episode: 158   episode steps: 260   reward: -44.9\n",
      "step 24757\n",
      "episode: 159   episode steps: 28    reward: -18.4\n",
      "step 24767\n",
      "episode: 160   episode steps: 10    reward: -3.2 \n",
      "step 24848\n",
      "episode: 161   episode steps: 81    reward: -24.7\n",
      "step 24879\n",
      "episode: 162   episode steps: 31    reward: -15.8\n",
      "step 24891\n",
      "episode: 163   episode steps: 12    reward: -10.6\n",
      "step 25010\n",
      "episode: 164   episode steps: 119   reward: -30.1\n",
      "step 25050\n",
      "episode: 165   episode steps: 40    reward: 17.0 \n",
      "step 26051\n",
      "episode: 166   episode steps: 1001  reward: -222.3\n",
      "step 26066\n",
      "episode: 167   episode steps: 15    reward: -7.4 \n",
      "step 26225\n",
      "episode: 168   episode steps: 159   reward: -53.8\n",
      "step 26276\n",
      "episode: 169   episode steps: 51    reward: -65.4\n",
      "step 27277\n",
      "episode: 170   episode steps: 1001  reward: -183.9\n",
      "step 27331\n",
      "episode: 171   episode steps: 54    reward: -10.0\n",
      "step 27447\n",
      "episode: 172   episode steps: 116   reward: -48.9\n",
      "step 27484\n",
      "episode: 173   episode steps: 37    reward: -25.2\n",
      "step 27684\n",
      "episode: 174   episode steps: 200   reward: 33.0 \n",
      "step 27863\n",
      "episode: 175   episode steps: 179   reward: -1.2 \n",
      "step 27879\n",
      "episode: 176   episode steps: 16    reward: 14.4 \n",
      "step 27914\n",
      "episode: 177   episode steps: 35    reward: -32.6\n",
      "step 27929\n",
      "episode: 178   episode steps: 15    reward: -0.2 \n",
      "step 28194\n",
      "episode: 179   episode steps: 265   reward: 57.7 \n",
      "step 28649\n",
      "episode: 180   episode steps: 455   reward: -67.4\n",
      "step 28734\n",
      "episode: 181   episode steps: 85    reward: 18.2 \n",
      "step 28772\n",
      "episode: 182   episode steps: 38    reward: -21.7\n",
      "step 28798\n",
      "episode: 183   episode steps: 26    reward: -2.8 \n",
      "step 28815\n",
      "episode: 184   episode steps: 17    reward: -4.0 \n",
      "step 28873\n",
      "episode: 185   episode steps: 58    reward: -52.4\n",
      "step 29049\n",
      "episode: 186   episode steps: 176   reward: -30.0\n",
      "step 29170\n",
      "episode: 187   episode steps: 121   reward: -28.7\n",
      "step 29213\n",
      "episode: 188   episode steps: 43    reward: -17.1\n",
      "step 29253\n",
      "episode: 189   episode steps: 40    reward: -8.8 \n",
      "step 29296\n",
      "episode: 190   episode steps: 43    reward: -38.3\n",
      "step 29429\n",
      "episode: 191   episode steps: 133   reward: -79.1\n",
      "step 29608\n",
      "episode: 192   episode steps: 179   reward: -29.6\n",
      "step 29676\n",
      "episode: 193   episode steps: 68    reward: -10.0\n",
      "step 29794\n",
      "episode: 194   episode steps: 118   reward: -41.3\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 30000  reward: 510.2\n",
      "------------------------------------------------------------\n",
      "step 30000\n",
      "episode: 195   episode steps: 206   reward: 14.7 \n",
      "step 30054\n",
      "episode: 196   episode steps: 54    reward: 10.4 \n",
      "step 30194\n",
      "episode: 197   episode steps: 140   reward: -2.6 \n",
      "step 30206\n",
      "episode: 198   episode steps: 12    reward: 5.7  \n",
      "step 30400\n",
      "episode: 199   episode steps: 194   reward: 102.0\n",
      "step 30423\n",
      "episode: 200   episode steps: 23    reward: -25.0\n",
      "step 30705\n",
      "episode: 201   episode steps: 282   reward: -8.3 \n",
      "step 30833\n",
      "episode: 202   episode steps: 128   reward: 25.0 \n",
      "step 30906\n",
      "episode: 203   episode steps: 73    reward: 6.1  \n",
      "step 30935\n",
      "episode: 204   episode steps: 29    reward: 15.9 \n",
      "step 30965\n",
      "episode: 205   episode steps: 30    reward: -22.6\n",
      "step 31026\n",
      "episode: 206   episode steps: 61    reward: -6.5 \n",
      "step 31158\n",
      "episode: 207   episode steps: 132   reward: 23.7 \n",
      "step 32159\n",
      "episode: 208   episode steps: 1001  reward: 55.5 \n",
      "step 32459\n",
      "episode: 209   episode steps: 300   reward: -40.6\n",
      "step 32670\n",
      "episode: 210   episode steps: 211   reward: 22.1 \n",
      "step 32679\n",
      "episode: 211   episode steps: 9     reward: 0.0  \n",
      "step 32699\n",
      "episode: 212   episode steps: 20    reward: -11.1\n",
      "step 32882\n",
      "episode: 213   episode steps: 183   reward: 24.0 \n",
      "step 33132\n",
      "episode: 214   episode steps: 250   reward: -21.5\n",
      "step 34133\n",
      "episode: 215   episode steps: 1001  reward: 63.8 \n",
      "step 34438\n",
      "episode: 216   episode steps: 305   reward: 3.8  \n",
      "step 34599\n",
      "episode: 217   episode steps: 161   reward: -8.1 \n",
      "step 34764\n",
      "episode: 218   episode steps: 165   reward: -0.3 \n",
      "step 35765\n",
      "episode: 219   episode steps: 1001  reward: 162.2\n",
      "step 36127\n",
      "episode: 220   episode steps: 362   reward: 13.7 \n",
      "step 36599\n",
      "episode: 221   episode steps: 472   reward: 70.0 \n",
      "step 36651\n",
      "episode: 222   episode steps: 52    reward: -27.0\n",
      "step 36707\n",
      "episode: 223   episode steps: 56    reward: 19.1 \n",
      "step 36758\n",
      "episode: 224   episode steps: 51    reward: -12.8\n",
      "step 36830\n",
      "episode: 225   episode steps: 72    reward: 6.2  \n",
      "step 37319\n",
      "episode: 226   episode steps: 489   reward: -27.0\n",
      "step 37463\n",
      "episode: 227   episode steps: 144   reward: 21.0 \n",
      "step 37522\n",
      "episode: 228   episode steps: 59    reward: -76.1\n",
      "step 37782\n",
      "episode: 229   episode steps: 260   reward: 34.5 \n",
      "step 37973\n",
      "episode: 230   episode steps: 191   reward: 37.2 \n",
      "step 38087\n",
      "episode: 231   episode steps: 114   reward: 11.4 \n",
      "step 39088\n",
      "episode: 232   episode steps: 1001  reward: 237.5\n",
      "step 39113\n",
      "episode: 233   episode steps: 25    reward: 7.0  \n",
      "step 39129\n",
      "episode: 234   episode steps: 16    reward: 0.0  \n",
      "step 39187\n",
      "episode: 235   episode steps: 58    reward: 21.2 \n",
      "step 39509\n",
      "episode: 236   episode steps: 322   reward: -15.9\n",
      "step 39686\n",
      "episode: 237   episode steps: 177   reward: 43.5 \n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 40000  reward: 883.2\n",
      "------------------------------------------------------------\n",
      "step 40000\n",
      "episode: 238   episode steps: 314   reward: 6.0  \n",
      "step 40373\n",
      "episode: 239   episode steps: 373   reward: 101.3\n",
      "step 40511\n",
      "episode: 240   episode steps: 138   reward: 12.8 \n",
      "step 40624\n",
      "episode: 241   episode steps: 113   reward: 12.7 \n",
      "step 40692\n",
      "episode: 242   episode steps: 68    reward: -20.8\n",
      "step 40804\n",
      "episode: 243   episode steps: 112   reward: 5.4  \n",
      "step 40819\n",
      "episode: 244   episode steps: 15    reward: 8.4  \n",
      "step 40906\n",
      "episode: 245   episode steps: 87    reward: 15.7 \n",
      "step 40967\n",
      "episode: 246   episode steps: 61    reward: -0.8 \n",
      "step 41968\n",
      "episode: 247   episode steps: 1001  reward: 120.3\n",
      "step 42174\n",
      "episode: 248   episode steps: 206   reward: 40.8 \n",
      "step 42699\n",
      "episode: 249   episode steps: 525   reward: -13.3\n",
      "step 42752\n",
      "episode: 250   episode steps: 53    reward: 13.7 \n",
      "step 42774\n",
      "episode: 251   episode steps: 22    reward: -0.7 \n",
      "step 42852\n",
      "episode: 252   episode steps: 78    reward: -35.5\n",
      "step 42957\n",
      "episode: 253   episode steps: 105   reward: -21.3\n",
      "step 43197\n",
      "episode: 254   episode steps: 240   reward: 98.7 \n",
      "step 43215\n",
      "episode: 255   episode steps: 18    reward: -0.2 \n",
      "step 44216\n",
      "episode: 256   episode steps: 1001  reward: 112.5\n",
      "step 45217\n",
      "episode: 257   episode steps: 1001  reward: 169.5\n",
      "step 45427\n",
      "episode: 258   episode steps: 210   reward: 3.8  \n",
      "step 45504\n",
      "episode: 259   episode steps: 77    reward: -16.8\n",
      "step 45550\n",
      "episode: 260   episode steps: 46    reward: -2.2 \n",
      "step 46359\n",
      "episode: 261   episode steps: 809   reward: 27.2 \n",
      "step 46691\n",
      "episode: 262   episode steps: 332   reward: 58.9 \n",
      "step 46737\n",
      "episode: 263   episode steps: 46    reward: 4.4  \n",
      "step 47738\n",
      "episode: 264   episode steps: 1001  reward: 154.3\n",
      "step 48739\n",
      "episode: 265   episode steps: 1001  reward: 169.0\n",
      "step 48765\n",
      "episode: 266   episode steps: 26    reward: -2.7 \n",
      "step 48982\n",
      "episode: 267   episode steps: 217   reward: 4.7  \n",
      "step 49366\n",
      "episode: 268   episode steps: 384   reward: 124.1\n",
      "step 49570\n",
      "episode: 269   episode steps: 204   reward: 49.7 \n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 50000  reward: 849.5\n",
      "------------------------------------------------------------\n",
      "step 50000\n",
      "episode: 270   episode steps: 430   reward: 155.4\n",
      "step 51001\n",
      "episode: 271   episode steps: 1001  reward: 199.5\n",
      "step 51052\n",
      "episode: 272   episode steps: 51    reward: 24.6 \n",
      "step 51065\n",
      "episode: 273   episode steps: 13    reward: -6.1 \n",
      "step 51168\n",
      "episode: 274   episode steps: 103   reward: 11.3 \n",
      "step 51204\n",
      "episode: 275   episode steps: 36    reward: -3.7 \n",
      "step 51589\n",
      "episode: 276   episode steps: 385   reward: 54.4 \n",
      "step 52590\n",
      "episode: 277   episode steps: 1001  reward: 362.4\n",
      "step 53591\n",
      "episode: 278   episode steps: 1001  reward: 249.0\n",
      "step 54592\n",
      "episode: 279   episode steps: 1001  reward: 164.8\n",
      "step 55593\n",
      "episode: 280   episode steps: 1001  reward: 157.1\n",
      "step 56161\n",
      "episode: 281   episode steps: 568   reward: 176.2\n",
      "step 56548\n",
      "episode: 282   episode steps: 387   reward: 140.0\n",
      "step 56686\n",
      "episode: 283   episode steps: 138   reward: 1.9  \n",
      "step 57423\n",
      "episode: 284   episode steps: 737   reward: 186.7\n",
      "step 58424\n",
      "episode: 285   episode steps: 1001  reward: 261.1\n",
      "step 59425\n",
      "episode: 286   episode steps: 1001  reward: 272.6\n",
      "step 59599\n",
      "episode: 287   episode steps: 174   reward: 77.4 \n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 60000  reward: 853.3\n",
      "------------------------------------------------------------\n",
      "step 60000\n",
      "episode: 288   episode steps: 401   reward: 11.3 \n",
      "step 60675\n",
      "episode: 289   episode steps: 675   reward: 294.1\n",
      "step 60869\n",
      "episode: 290   episode steps: 194   reward: 28.3 \n",
      "step 61870\n",
      "episode: 291   episode steps: 1001  reward: 201.0\n",
      "step 62380\n",
      "episode: 292   episode steps: 510   reward: 148.6\n",
      "step 62392\n",
      "episode: 293   episode steps: 12    reward: 4.5  \n",
      "step 63239\n",
      "episode: 294   episode steps: 847   reward: 298.4\n",
      "step 63302\n",
      "episode: 295   episode steps: 63    reward: 3.9  \n",
      "step 64303\n",
      "episode: 296   episode steps: 1001  reward: 222.6\n",
      "step 65304\n",
      "episode: 297   episode steps: 1001  reward: 333.8\n",
      "step 65476\n",
      "episode: 298   episode steps: 172   reward: 98.0 \n",
      "step 66477\n",
      "episode: 299   episode steps: 1001  reward: 305.6\n",
      "step 66495\n",
      "episode: 300   episode steps: 18    reward: -20.2\n",
      "step 66693\n",
      "episode: 301   episode steps: 198   reward: 87.9 \n",
      "step 66884\n",
      "episode: 302   episode steps: 191   reward: 31.4 \n",
      "step 67027\n",
      "episode: 303   episode steps: 143   reward: 20.0 \n",
      "step 68028\n",
      "episode: 304   episode steps: 1001  reward: 308.5\n",
      "step 68114\n",
      "episode: 305   episode steps: 86    reward: 34.7 \n",
      "step 69115\n",
      "episode: 306   episode steps: 1001  reward: 558.0\n",
      "step 69533\n",
      "episode: 307   episode steps: 418   reward: 223.5\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 70000  reward: 888.6\n",
      "------------------------------------------------------------\n",
      "step 70000\n",
      "episode: 308   episode steps: 467   reward: 220.7\n",
      "step 70053\n",
      "episode: 309   episode steps: 53    reward: 12.8 \n",
      "step 70553\n",
      "episode: 310   episode steps: 500   reward: 225.9\n",
      "step 70837\n",
      "episode: 311   episode steps: 284   reward: 113.8\n",
      "step 71322\n",
      "episode: 312   episode steps: 485   reward: 180.2\n",
      "step 72323\n",
      "episode: 313   episode steps: 1001  reward: 404.0\n",
      "step 73079\n",
      "episode: 314   episode steps: 756   reward: 369.7\n",
      "step 73976\n",
      "episode: 315   episode steps: 897   reward: 304.9\n",
      "step 74977\n",
      "episode: 316   episode steps: 1001  reward: 529.0\n",
      "step 75067\n",
      "episode: 317   episode steps: 90    reward: 63.8 \n",
      "step 76068\n",
      "episode: 318   episode steps: 1001  reward: 461.3\n",
      "step 76543\n",
      "episode: 319   episode steps: 475   reward: 162.3\n",
      "step 77544\n",
      "episode: 320   episode steps: 1001  reward: 532.8\n",
      "step 78545\n",
      "episode: 321   episode steps: 1001  reward: 472.8\n",
      "step 79069\n",
      "episode: 322   episode steps: 524   reward: 276.5\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 80000  reward: 927.7\n",
      "------------------------------------------------------------\n",
      "step 80000\n",
      "episode: 323   episode steps: 931   reward: 387.7\n",
      "step 80462\n",
      "episode: 324   episode steps: 462   reward: 270.3\n",
      "step 81463\n",
      "episode: 325   episode steps: 1001  reward: 514.9\n",
      "step 82464\n",
      "episode: 326   episode steps: 1001  reward: 588.0\n",
      "step 83465\n",
      "episode: 327   episode steps: 1001  reward: 468.9\n",
      "step 84466\n",
      "episode: 328   episode steps: 1001  reward: 476.3\n",
      "step 85176\n",
      "episode: 329   episode steps: 710   reward: 335.0\n",
      "step 86177\n",
      "episode: 330   episode steps: 1001  reward: 524.2\n",
      "step 87178\n",
      "episode: 331   episode steps: 1001  reward: 357.6\n",
      "step 87791\n",
      "episode: 332   episode steps: 613   reward: 170.2\n",
      "step 88792\n",
      "episode: 333   episode steps: 1001  reward: 432.1\n",
      "step 89793\n",
      "episode: 334   episode steps: 1001  reward: 444.6\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 90000  reward: 837.9\n",
      "------------------------------------------------------------\n",
      "step 90000\n",
      "episode: 335   episode steps: 207   reward: 74.3 \n",
      "step 91001\n",
      "episode: 336   episode steps: 1001  reward: 144.5\n",
      "step 92002\n",
      "episode: 337   episode steps: 1001  reward: 157.5\n",
      "step 93003\n",
      "episode: 338   episode steps: 1001  reward: 1.0  \n",
      "step 94004\n",
      "episode: 339   episode steps: 1001  reward: -93.3\n",
      "step 95005\n",
      "episode: 340   episode steps: 1001  reward: -159.9\n",
      "step 96006\n",
      "episode: 341   episode steps: 1001  reward: -36.5\n",
      "step 97007\n",
      "episode: 342   episode steps: 1001  reward: -22.5\n",
      "step 98008\n",
      "episode: 343   episode steps: 1001  reward: -123.5\n",
      "step 99009\n",
      "episode: 344   episode steps: 1001  reward: -71.9\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 100000  reward: 530.0\n",
      "------------------------------------------------------------\n",
      "step 100000\n",
      "episode: 345   episode steps: 991   reward: 24.1 \n",
      "step 101001\n",
      "episode: 346   episode steps: 1001  reward: 431.8\n",
      "step 101810\n",
      "episode: 347   episode steps: 809   reward: 389.6\n",
      "step 102537\n",
      "episode: 348   episode steps: 727   reward: 24.7 \n",
      "step 103538\n",
      "episode: 349   episode steps: 1001  reward: -279.6\n",
      "step 104539\n",
      "episode: 350   episode steps: 1001  reward: 443.7\n",
      "step 105134\n",
      "episode: 351   episode steps: 595   reward: -16.2\n",
      "step 106135\n",
      "episode: 352   episode steps: 1001  reward: 272.8\n",
      "step 107136\n",
      "episode: 353   episode steps: 1001  reward: -149.6\n",
      "step 108137\n",
      "episode: 354   episode steps: 1001  reward: 490.6\n",
      "step 109138\n",
      "episode: 355   episode steps: 1001  reward: 96.9 \n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 110000  reward: 790.3\n",
      "------------------------------------------------------------\n",
      "step 110000\n",
      "episode: 356   episode steps: 862   reward: 480.2\n",
      "step 110250\n",
      "episode: 357   episode steps: 250   reward: 131.4\n",
      "step 111251\n",
      "episode: 358   episode steps: 1001  reward: 646.9\n",
      "step 112252\n",
      "episode: 359   episode steps: 1001  reward: 242.0\n",
      "step 113253\n",
      "episode: 360   episode steps: 1001  reward: 534.6\n",
      "step 114254\n",
      "episode: 361   episode steps: 1001  reward: 503.4\n",
      "step 115255\n",
      "episode: 362   episode steps: 1001  reward: 461.7\n",
      "step 116256\n",
      "episode: 363   episode steps: 1001  reward: 572.3\n",
      "step 117257\n",
      "episode: 364   episode steps: 1001  reward: 484.5\n",
      "step 118258\n",
      "episode: 365   episode steps: 1001  reward: 630.8\n",
      "step 119259\n",
      "episode: 366   episode steps: 1001  reward: 674.0\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 120000  reward: 882.8\n",
      "------------------------------------------------------------\n",
      "step 120000\n",
      "episode: 367   episode steps: 741   reward: 604.5\n",
      "step 121001\n",
      "episode: 368   episode steps: 1001  reward: 726.9\n",
      "step 122002\n",
      "episode: 369   episode steps: 1001  reward: 721.5\n",
      "step 123003\n",
      "episode: 370   episode steps: 1001  reward: 654.7\n",
      "step 124004\n",
      "episode: 371   episode steps: 1001  reward: 646.0\n",
      "step 125005\n",
      "episode: 372   episode steps: 1001  reward: 511.5\n",
      "step 126006\n",
      "episode: 373   episode steps: 1001  reward: 550.6\n",
      "step 127007\n",
      "episode: 374   episode steps: 1001  reward: 521.6\n",
      "step 128008\n",
      "episode: 375   episode steps: 1001  reward: 603.7\n",
      "step 129009\n",
      "episode: 376   episode steps: 1001  reward: 498.6\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 130000  reward: 676.6\n",
      "------------------------------------------------------------\n",
      "step 130000\n",
      "episode: 377   episode steps: 991   reward: 388.7\n",
      "step 130015\n",
      "episode: 378   episode steps: 15    reward: -4.0 \n",
      "step 131016\n",
      "episode: 379   episode steps: 1001  reward: 402.4\n",
      "step 132017\n",
      "episode: 380   episode steps: 1001  reward: 308.4\n",
      "step 133018\n",
      "episode: 381   episode steps: 1001  reward: 185.1\n",
      "step 134019\n",
      "episode: 382   episode steps: 1001  reward: 159.4\n",
      "step 135020\n",
      "episode: 383   episode steps: 1001  reward: 14.1 \n",
      "step 136021\n",
      "episode: 384   episode steps: 1001  reward: 50.8 \n",
      "step 137022\n",
      "episode: 385   episode steps: 1001  reward: -23.4\n",
      "step 138023\n",
      "episode: 386   episode steps: 1001  reward: -146.8\n",
      "step 139024\n",
      "episode: 387   episode steps: 1001  reward: -129.0\n",
      "eva\n",
      "eva_i 0\n",
      "eva_i 1\n",
      "eva_i 2\n",
      "eva_i 3\n",
      "eva_i 4\n",
      "eva_i 5\n",
      "eva_i 6\n",
      "eva_i 7\n",
      "eva_i 8\n",
      "eva_i 9\n",
      "------------------------------------------------------------\n",
      "Num steps: 140000  reward: -164.6\n",
      "------------------------------------------------------------\n",
      "step 140000\n",
      "episode: 388   episode steps: 976   reward: -47.3\n",
      "step 141001\n",
      "episode: 389   episode steps: 1001  reward: -305.2\n",
      "step 142002\n",
      "episode: 390   episode steps: 1001  reward: -141.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f211f7f9cd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/mujoco/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "episodes = 0\n",
    "state_shape={}\n",
    "while steps < num_steps:\n",
    "    episode_reward = 0.\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    episodes += 1\n",
    "    state=state[0]\n",
    "    start=0\n",
    "    while not done and start<=1000:\n",
    "        start+=1\n",
    "        if start_steps > steps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.sample_action(state, eval=False)\n",
    "        next_state, reward, done, _ ,info= env.step(action)\n",
    "\n",
    "        mask = 0.0 if done else args.gamma\n",
    "\n",
    "        steps += 1\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        agent.append_memory(state[0], action, reward, next_state, mask)\n",
    "\n",
    "        if steps >= start_steps:\n",
    "            #print(\"train\")\n",
    "            agent.train(updates_per_step, batch_size=batch_size, log_writer=writer)\n",
    "\n",
    "        if steps % eval_interval == 0:\n",
    "            evaluate(env, agent, writer, steps)\n",
    "            # self.save_models()\n",
    "            done =True\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # if episodes % log_interval == 0:\n",
    "    #     writer.add_scalar('reward/train', episode_reward, steps)\n",
    "    print(\"step\",steps)\n",
    "    print(f'episode: {episodes:<4}  '\n",
    "        f'episode steps: {episode_steps:<4}  '\n",
    "        f'reward: {episode_reward:<5.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb70b98-1e9d-4ef6-b7dd-9ba73cfda51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbe325-e7ee-4799-a1a5-6c87ee72930f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c96b3cb-fe41-4469-8b04-50436ea8aea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fca8bd-f2d2-4aaa-81ac-6d3f611be8f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2706791-5a2e-4564-a3ce-4b41b3226a77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41524802225522506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4744da-8e09-479f-9e6f-47dca1f2995a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39midx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "self.memory.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f9be6-5071-4820-bad6-17dcef02b281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mujoco",
   "name": "tf2-cpu.2-11.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m123"
  },
  "kernelspec": {
   "display_name": "mujoco (Local)",
   "language": "python",
   "name": "mujoco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
